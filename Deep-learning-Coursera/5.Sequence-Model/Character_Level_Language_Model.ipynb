{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)\n",
    "\n",
    "def smooth(loss, cur_loss):\n",
    "    return loss * 0.999 + cur_loss * 0.001\n",
    "\n",
    "def print_sample(sample_ix, ix_to_char):\n",
    "    txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "    txt = txt[0].upper() + txt[1:]  # capitalize first character \n",
    "    print ('%s' % (txt, ), end='')\n",
    "\n",
    "def get_initial_loss(vocab_size, seq_length):\n",
    "    return -np.log(1.0/vocab_size)*seq_length\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)\n",
    "\n",
    "def initialize_parameters(n_a, n_x, n_y):\n",
    "\n",
    "    np.random.seed(1)\n",
    "    Wax = np.random.randn(n_a, n_x)*0.01 \n",
    "    Waa = np.random.randn(n_a, n_a)*0.01 \n",
    "    Wya = np.random.randn(n_y, n_a)*0.01 \n",
    "    b = np.zeros((n_a, 1)) \n",
    "    by = np.zeros((n_y, 1)) \n",
    "    \n",
    "    parameters = {\"Wax\": Wax, \"Waa\": Waa, \"Wya\": Wya, \"b\": b,\"by\": by}\n",
    "    \n",
    "    return parameters\n",
    "\n",
    "def rnn_step_forward(parameters, a_prev, x):\n",
    "    \n",
    "    Waa, Wax, Wya, by, b = parameters['Waa'], parameters['Wax'], parameters['Wya'], parameters['by'], parameters['b']\n",
    "    a_next = np.tanh(np.dot(Wax, x) + np.dot(Waa, a_prev) + b) # hidden state\n",
    "    p_t = softmax(np.dot(Wya, a_next) + by) # unnormalized log probabilities for next chars # probabilities for next chars \n",
    "    \n",
    "    return a_next, p_t\n",
    "\n",
    "def rnn_step_backward(dy, gradients, parameters, x, a, a_prev):\n",
    "    \n",
    "    gradients['dWya'] += np.dot(dy, a.T)\n",
    "    gradients['dby'] += dy\n",
    "    da = np.dot(parameters['Wya'].T, dy) + gradients['da_next'] # backprop into h\n",
    "    daraw = (1 - a * a) * da # backprop through tanh nonlinearity\n",
    "    gradients['db'] += daraw\n",
    "    gradients['dWax'] += np.dot(daraw, x.T)\n",
    "    gradients['dWaa'] += np.dot(daraw, a_prev.T)\n",
    "    gradients['da_next'] = np.dot(parameters['Waa'].T, daraw)\n",
    "    return gradients\n",
    "\n",
    "def update_parameters(parameters, gradients, lr):\n",
    "\n",
    "    parameters['Wax'] += -lr * gradients['dWax']\n",
    "    parameters['Waa'] += -lr * gradients['dWaa']\n",
    "    parameters['Wya'] += -lr * gradients['dWya']\n",
    "    parameters['b']  += -lr * gradients['db']\n",
    "    parameters['by']  += -lr * gradients['dby']\n",
    "    return parameters\n",
    "\n",
    "def rnn_forward(X, Y, a0, parameters, vocab_size = 27):\n",
    "\n",
    "    x, a, y_hat = {}, {}, {}\n",
    "    a[-1] = np.copy(a0)\n",
    "    loss = 0\n",
    "    \n",
    "    for t in range(len(X)):\n",
    "        x[t] = np.zeros((vocab_size,1)) \n",
    "        if (X[t] != None):\n",
    "            x[t][X[t]] = 1\n",
    "        \n",
    "        a[t], y_hat[t] = rnn_step_forward(parameters, a[t-1], x[t])\n",
    "        loss -= np.log(y_hat[t][Y[t],0])\n",
    "        \n",
    "    cache = (y_hat, a, x)\n",
    "        \n",
    "    return loss, cache\n",
    "\n",
    "def rnn_backward(X, Y, parameters, cache):\n",
    "\n",
    "    gradients = {}\n",
    "    (y_hat, a, x) = cache\n",
    "    Waa, Wax, Wya, by, b = parameters['Waa'], parameters['Wax'], parameters['Wya'], parameters['by'], parameters['b']\n",
    "    \n",
    "    gradients['dWax'], gradients['dWaa'], gradients['dWya'] = np.zeros_like(Wax), np.zeros_like(Waa), np.zeros_like(Wya)\n",
    "    gradients['db'], gradients['dby'] = np.zeros_like(b), np.zeros_like(by)\n",
    "    gradients['da_next'] = np.zeros_like(a[0])\n",
    "\n",
    "    for t in reversed(range(len(X))):\n",
    "        dy = np.copy(y_hat[t])\n",
    "        dy[Y[t]] -= 1\n",
    "        gradients = rnn_step_backward(dy, gradients, parameters, x[t], a[t], a[t-1])\n",
    "    \n",
    "    return gradients, a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 梯度修剪\n",
    "\n",
    "def clip(gradients, maxValue):\n",
    "    \n",
    "    dWaa, dWax, dWya, db, dby = gradients['dWaa'], gradients['dWax'], gradients['dWya'], gradients['db'], gradients['dby']\n",
    "    for gradient in [dWax, dWaa, dWya, db, dby]:\n",
    "        np.clip(gradient, -maxValue, maxValue, out = gradient)\n",
    "    \n",
    "    gradients = {\"dWaa\": dWaa, \"dWax\": dWax, \"dWya\": dWya, \"db\": db, \"dby\": dby}\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 采样\n",
    "\n",
    "def sample(parameters, char_to_ix, seed):\n",
    "\n",
    "    Waa, Wax, Wya, by, b = parameters['Waa'], parameters['Wax'], parameters['Wya'], parameters['by'], parameters['b']\n",
    "    vocab_size = by.shape[0]\n",
    "    n_a = Waa.shape[1]\n",
    "    \n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    a_prev = np.zeros((n_a, 1))\n",
    "    \n",
    "    indices = []\n",
    "    idx = -1 \n",
    "\n",
    "    counter = 0\n",
    "    newline_character = char_to_ix['\\n']\n",
    "    \n",
    "    while (idx != newline_character and counter != 50):\n",
    "        \n",
    "        a = np.tanh(np.dot(Wax, x) + np.dot(Waa, a_prev) + b)\n",
    "        z = np.dot(Wya, a) + by\n",
    "        y = softmax(z)\n",
    "        \n",
    "        np.random.seed(counter+seed) \n",
    "        idx = np.random.choice(range(len(y)), p = y.ravel())\n",
    "        indices.append(idx)\n",
    " \n",
    "        x = np.zeros((vocab_size, 1))\n",
    "        x[idx] = 1\n",
    "        a_prev = a\n",
    "        \n",
    "        seed += 1\n",
    "        counter +=1\n",
    "\n",
    "    if (counter == 50):\n",
    "        indices.append(char_to_ix['\\n'])\n",
    "    \n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#建模\n",
    "\n",
    "def optimize(X, Y, a_prev, parameters, learning_rate = 0.01):\n",
    " \n",
    "    loss, cache = rnn_forward(X, Y, a_prev, parameters)\n",
    "    gradients, a = rnn_backward(X, Y, parameters, cache)\n",
    "    gradients =  clip(gradients, 5)\n",
    "    parameters = update_parameters(parameters, gradients, learning_rate)\n",
    "    \n",
    "    return loss, gradients, a[len(X)-1]\n",
    "\n",
    "def model(data, ix_to_char, char_to_ix, num_iterations = 35000, n_a = 50, dino_names = 7, vocab_size = 27):\n",
    "\n",
    "    n_x, n_y = vocab_size, vocab_size\n",
    "    parameters = initialize_parameters(n_a, n_x, n_y)\n",
    "    loss = get_initial_loss(vocab_size, dino_names)\n",
    "    \n",
    "    with open(\"data\\dinos.txt\") as f:\n",
    "        examples = f.readlines()\n",
    "    examples = [x.lower().strip() for x in examples]\n",
    "    \n",
    "    np.random.seed(0)\n",
    "    np.random.shuffle(examples)\n",
    "    \n",
    "    a_prev = np.zeros((n_a, 1))\n",
    "    \n",
    "    for j in range(num_iterations):\n",
    "        index = j % len(examples)\n",
    "        X = [None] + [char_to_ix[ch] for ch in examples[index]] \n",
    "        Y = X[1:] + [char_to_ix[\"\\n\"]]\n",
    "        \n",
    "        curr_loss, gradients, a_prev = optimize(X, Y, a_prev, parameters, learning_rate = 0.01)\n",
    "        loss = smooth(loss, curr_loss)\n",
    "\n",
    "        if j % 2000 == 0:\n",
    "            print('Iteration: %d, Loss: %f' % (j, loss) + '\\n')\n",
    "            seed = 0\n",
    "            for name in range(dino_names):\n",
    "                sampled_indices = sample(parameters, char_to_ix, seed)\n",
    "                print_sample(sampled_indices, ix_to_char)\n",
    "                seed += 1  \n",
    "                \n",
    "            print('\\n')\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0, Loss: 23.087336\n",
      "\n",
      "Nkzxwtdmfqoeyhsqwasjkjvu\n",
      "Kneb\n",
      "Kzxwtdmfqoeyhsqwasjkjvu\n",
      "Neb\n",
      "Zxwtdmfqoeyhsqwasjkjvu\n",
      "Eb\n",
      "Xwtdmfqoeyhsqwasjkjvu\n",
      "\n",
      "\n",
      "Iteration: 2000, Loss: 27.884160\n",
      "\n",
      "Liusskeomnolxeros\n",
      "Hmdaairus\n",
      "Hytroligoraurus\n",
      "Lecalosapaus\n",
      "Xusicikoraurus\n",
      "Abalpsamantisaurus\n",
      "Tpraneronxeros\n",
      "\n",
      "\n",
      "Iteration: 4000, Loss: 25.901815\n",
      "\n",
      "Mivrosaurus\n",
      "Inee\n",
      "Ivtroplisaurus\n",
      "Mbaaisaurus\n",
      "Wusichisaurus\n",
      "Cabaselachus\n",
      "Toraperlethosdarenitochusthiamamumamaon\n",
      "\n",
      "\n",
      "Iteration: 6000, Loss: 24.608779\n",
      "\n",
      "Onwusceomosaurus\n",
      "Lieeaerosaurus\n",
      "Lxussaurus\n",
      "Oma\n",
      "Xusteonosaurus\n",
      "Eeahosaurus\n",
      "Toreonosaurus\n",
      "\n",
      "\n",
      "Iteration: 8000, Loss: 24.070350\n",
      "\n",
      "Onxusichepriuon\n",
      "Kilabersaurus\n",
      "Lutrodon\n",
      "Omaaerosaurus\n",
      "Xutrcheps\n",
      "Edaksoje\n",
      "Trodiktonus\n",
      "\n",
      "\n",
      "Iteration: 10000, Loss: 23.844446\n",
      "\n",
      "Onyusaurus\n",
      "Klecalosaurus\n",
      "Lustodon\n",
      "Ola\n",
      "Xusodonia\n",
      "Eeaeosaurus\n",
      "Troceosaurus\n",
      "\n",
      "\n",
      "Iteration: 12000, Loss: 23.291971\n",
      "\n",
      "Onyxosaurus\n",
      "Kica\n",
      "Lustrepiosaurus\n",
      "Olaagrraiansaurus\n",
      "Yuspangosaurus\n",
      "Eealosaurus\n",
      "Trognesaurus\n",
      "\n",
      "\n",
      "Iteration: 14000, Loss: 23.382338\n",
      "\n",
      "Meutromodromurus\n",
      "Inda\n",
      "Iutroinatorsaurus\n",
      "Maca\n",
      "Yusteratoptititan\n",
      "Ca\n",
      "Troclosaurus\n",
      "\n",
      "\n",
      "Iteration: 16000, Loss: 23.268257\n",
      "\n",
      "Mbutosaurus\n",
      "Indaa\n",
      "Iustolophulurus\n",
      "Macagosaurus\n",
      "Yusoclichaurus\n",
      "Caahosaurus\n",
      "Trodon\n",
      "\n",
      "\n",
      "Iteration: 18000, Loss: 22.928870\n",
      "\n",
      "Phytrogiaps\n",
      "Mela\n",
      "Mustrha\n",
      "Pegamosaurus\n",
      "Ytromacisaurus\n",
      "Efanshie\n",
      "Troma\n",
      "\n",
      "\n",
      "Iteration: 20000, Loss: 23.008798\n",
      "\n",
      "Onyusperchohychus\n",
      "Lola\n",
      "Lytrranfosaurus\n",
      "Olaa\n",
      "Ytrrcharomulus\n",
      "Ehagosaurus\n",
      "Trrcharonyhus\n",
      "\n",
      "\n",
      "Iteration: 22000, Loss: 22.794515\n",
      "\n",
      "Onyvus\n",
      "Llecakosaurus\n",
      "Mustodonosaurus\n",
      "Ola\n",
      "Yusodon\n",
      "Eiadosaurus\n",
      "Trodontorus\n",
      "\n",
      "\n",
      "Iteration: 24000, Loss: 22.648635\n",
      "\n",
      "Meutosaurus\n",
      "Incaachudachus\n",
      "Itntodon\n",
      "Mecaessan\n",
      "Yurong\n",
      "Daadropachusaurus\n",
      "Troenatheusaurosaurus\n",
      "\n",
      "\n",
      "Iteration: 26000, Loss: 22.599152\n",
      "\n",
      "Nixusehoenomulushapnelspanthuonathitalia\n",
      "Jigaadroncansaurus\n",
      "Kustodonis\n",
      "Nedantrocantiteniupegyankuaeusalomarotimenmpangvin\n",
      "Ytrodongoluctos\n",
      "Eebdssaegoterichus\n",
      "Trodolopiunsitarbilus\n",
      "\n",
      "\n",
      "Iteration: 28000, Loss: 22.628455\n",
      "\n",
      "Pnywrodilosaurus\n",
      "Loca\n",
      "Mustodonanethosaurus\n",
      "Phabesceeatopsaurus\n",
      "Ytrodonnoludosaurus\n",
      "Elaishacaosaurus\n",
      "Trrdilosaurus\n",
      "\n",
      "\n",
      "Iteration: 30000, Loss: 22.587893\n",
      "\n",
      "Piusosaurus\n",
      "Locaadrus\n",
      "Lutosaurus\n",
      "Pacalosaurus\n",
      "Yusochesaurus\n",
      "Eg\n",
      "Trraodon\n",
      "\n",
      "\n",
      "Iteration: 32000, Loss: 22.314649\n",
      "\n",
      "Nivosaurus\n",
      "Jiacamisaurus\n",
      "Kusplasaurus\n",
      "Ncaadosaurus\n",
      "Yusiandon\n",
      "Eeaisilaanus\n",
      "Trokalenator\n",
      "\n",
      "\n",
      "Iteration: 34000, Loss: 22.445100\n",
      "\n",
      "Mewsroengosaurus\n",
      "Ilabafosaurus\n",
      "Justoeomimavesaurus\n",
      "Macaeosaurus\n",
      "Yrosaurus\n",
      "Eiaeosaurus\n",
      "Trodondolus\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = open('data\\dinos.txt', 'r').read()\n",
    "data= data.lower()\n",
    "chars = list(set(data))\n",
    "char_to_ix = { ch:i for i,ch in enumerate(sorted(chars)) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(sorted(chars)) }\n",
    "parameters = model(data, ix_to_char, char_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from keras.callbacks import LambdaCallback\n",
    "from keras.models import Model, load_model, Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Input, Masking\n",
    "from keras.layers import LSTM\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_data(text, Tx = 40, stride = 3):\n",
    "    \n",
    "    X = []\n",
    "    Y = []\n",
    "\n",
    "    for i in range(0, len(text) - Tx, stride):\n",
    "        X.append(text[i: i + Tx])\n",
    "        Y.append(text[i + Tx])\n",
    "    \n",
    "    print('number of training examples:', len(X))\n",
    "    \n",
    "    return X, Y\n",
    "\n",
    "\n",
    "def vectorization(X, Y, n_x, char_indices, Tx = 40):\n",
    "    \n",
    "    m = len(X)\n",
    "    x = np.zeros((m, Tx, n_x), dtype=np.bool)\n",
    "    y = np.zeros((m, n_x), dtype=np.bool)\n",
    "    for i, sentence in enumerate(X):\n",
    "        for t, char in enumerate(sentence):\n",
    "            x[i, t, char_indices[char]] = 1\n",
    "        y[i, char_indices[Y[i]]] = 1\n",
    "        \n",
    "    return x, y \n",
    "\n",
    "\n",
    "def sample(preds, temperature=1.0):\n",
    "\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    out = np.random.choice(range(len(chars)), p = probas.ravel())\n",
    "    \n",
    "    return out\n",
    "\n",
    "def on_epoch_end(epoch, logs):\n",
    "    # Function invoked at end of each epoch. Prints generated text.\n",
    "    None\n",
    "\n",
    "def generate_output():\n",
    "    \n",
    "    generated = ''\n",
    "    usr_input = input(\"Write the beginning of your poem, the Shakespeare machine will complete it. Your input is: \")\n",
    "    sentence = ('{0:0>' + str(Tx) + '}').format(usr_input).lower()\n",
    "    generated += usr_input \n",
    "\n",
    "    sys.stdout.write(\"\\n\\nHere is your poem: \\n\\n\") \n",
    "    sys.stdout.write(usr_input)\n",
    "    for i in range(400):\n",
    "\n",
    "        x_pred = np.zeros((1, Tx, len(chars)))\n",
    "\n",
    "        for t, char in enumerate(sentence):\n",
    "            if char != '0':\n",
    "                x_pred[0, t, char_indices[char]] = 1.\n",
    "\n",
    "        preds = model.predict(x_pred, verbose=0)[0]\n",
    "        next_index = sample(preds, temperature = 1.0)\n",
    "        next_char = indices_char[next_index]\n",
    "\n",
    "        generated += next_char\n",
    "        sentence = sentence[1:] + next_char\n",
    "\n",
    "        sys.stdout.write(next_char)\n",
    "        sys.stdout.flush()\n",
    "\n",
    "        if next_char == '\\n':\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading text data...\n",
      "Creating training set...\n",
      "number of training examples: 31412\n",
      "Vectorizing training set...\n",
      "Loading model...\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading text data...\")\n",
    "text = io.open('data\\shakespeare.txt', encoding='utf-8').read().lower()\n",
    "\n",
    "Tx = 40\n",
    "chars = sorted(list(set(text)))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "print(\"Creating training set...\")\n",
    "X, Y = build_data(text, Tx, stride = 3)\n",
    "print(\"Vectorizing training set...\")\n",
    "x, y = vectorization(X, Y, n_x = len(chars), char_indices = char_indices) \n",
    "print(\"Loading model...\")\n",
    "model = load_model('model/model_shakespeare_kiank_350_epoch.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "31412/31412 [==============================] - 154s 5ms/step - loss: 2.5600\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2ba34211278>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
    "model.fit(x, y, batch_size=128, epochs=1, callbacks=[print_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write the beginning of your poem, the Shakespeare machine will complete it. Your input is: call me by your name\n",
      "\n",
      "\n",
      "Here is your poem: \n",
      "\n",
      "call me by your name,\n",
      "now hapty love ever suce deare,\n",
      "bit ribusges to parlitheos should all thing''s stence not,\n",
      "she same the but i saan the and theeing serls,\n",
      "hade dewins hangys or lo(king wat perpote..\n",
      "hoth loves glow tenmed whit ence wher for chears,\n",
      "and tosway of buy, beser nith mase,\n",
      "mingh she precetlabcaind nof savan then hid sprind undeense.\n",
      "bount is thint meliis maser didhrire to farots ar ont:\n",
      "who ca not fab"
     ]
    }
   ],
   "source": [
    "generate_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
